{"title":"Exploring Probabilities with Monte Carlo Simulations","markdown":{"yaml":{"title":"Exploring Probabilities with Monte Carlo Simulations","author":"Stephen Owesney","date":"2023-12-13","categories":["code","ML","probability","monte carlo"],"image":"data.png"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nIn the context of probability theory, random variables, and engineering in general, Monte Carlo simulations stand as powerful navigational tools, offering a unique lens through which we can explore and understand the uncertainties inherent in various phenomena. Named after the illustrious Monte Carlo Casino in Monaco, where chance plays a central role, these simulations harness the power of randomness to estimate probabilities and model complex systems. Through the art of random sampling, we'll unravel the mysteries of chance, casting a digital die to uncover patterns, distributions, and showcase how this technique can be used to approximate values that are otherwise vary difficult to ascertain.\n\n## Monte Carlo Simulation Simple Demonstration\n\nNow, let's embark on a hands-on journey to demystify the magic of probability theory and random variables using a simple yet powerful example: estimating the probability of getting heads in a fair coin toss. \n\nPicture yourself holding a fair coin. When you flip it, the outcome is uncertain – it could be heads or tails, a classic example of a random variable. The intrigue lies in understanding the behavior of this coin, and that's where probability theory steps in.\n\nIn the world of probability theory, we're curious about the likelihood of specific outcomes. Here, our burning question is: What's the probability of landing heads in a single toss? This is where Monte Carlo simulations come into play – a dynamic tool that transforms uncertainty into insight.\n\nImagine taking that fair coin and flipping it not just once, but thousands of times in a virtual environment. Each flip is like a mini-experiment, and the more experiments we conduct, the better we can grasp the elusive nature of our random variable – the coin toss. Monte Carlo simulations let us explore this randomness by simulating scenarios, recording outcomes, and gradually revealing the underlying probabilities.\n\nHere we run the simulation of the coin toss 10,000 times resulting in a relatively accurate estimation of what the probability should theoretically be.\n\n\n<p style='font-size:23px'>\n&emsp;&emsp; \n</p>\n``` {python}\n#| code-fold: true\nimport numpy as np\n\n# Function to simulate a coin toss\ndef simulate_coin_toss(num_simulations):\n    outcomes = np.random.choice(['Heads', 'Tails'], size=num_simulations)\n    heads_count = np.sum(outcomes == 'Heads')\n    probability_heads = heads_count / num_simulations\n    return probability_heads\n\n# Number of simulations\nnum_simulations = 10000\n\n# Run the Monte Carlo simulation\nestimated_probability = simulate_coin_toss(num_simulations)\n\nprint(f\"Estimated Probability of Heads: {estimated_probability}\")\n```\n<p style='font-size:23px'>\n\nNow, let's visualize how our estimated probability converges to the true theoretical probability as we increase the number of simulations. In this demonstration, the number of simulations increases by an order of magnitude each iteration which can be more approachably shown by graphing the x-axis on a logarithmic scale. \n\n``` {python}\n#| code-fold: true\nimport matplotlib.pyplot as plt\n\n# Function to run Monte Carlo simulations with varying sample sizes\ndef run_simulations():\n    num_simulations_list = [10, 100, 1000, 10000, 100000, 1000000]\n    probabilities = []\n\n    for num_simulations in num_simulations_list:\n        estimated_probability = simulate_coin_toss(num_simulations)\n        probabilities.append(estimated_probability)\n\n    return num_simulations_list, probabilities\n\n# Run simulations\nnum_simulations_list, probabilities = run_simulations()\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.xscale('log')\nplt.plot(num_simulations_list, probabilities, marker='o', linestyle='-', color='b')\nplt.axhline(y=0.5, color='r', linestyle='--', label='Theoretical Probability (0.5)')\nplt.title('Convergence of Monte Carlo Simulation to Theoretical Probability')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Estimated Probability of Heads')\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n<p style='font-size:23px'>\n\nIt is important to note that everytime you run a monte carlo simulation, the results will vary due to the stochastic nature of the process. Above you should see that, generally, as the number of simulations increases so does the accuracy of the estimation. However, other factors may affect the convergence of the estimate such as initial biases, statistical variability, and the nature of the specific problem you are applying the algorithm to which may skew the reliability of the results. \n\n## Monte Carlo Simulation: Estimating Circle Area\n\nNext, we can apply a monte carlo simulation technique to a more interesting problem: estimating the area of a shape. This could be useful in situations where you have a space and a subspace and it is difficult or unideal to model their relationship mathematically. In general, you can generate random points within the space and use the ratio of points that exist in the subspace to the points that exist in the entire space in order to estimate the dimensions of the subspace. For sake of simpilicity, here we can demonstrate this for a circle.\n\nConsider a circle with radius.. $$ r = 1 $$ ..centered at the origin. The formula for the area of a circle is given by..  $$ A = pi*r^2 $$ We can estimate this area by generating random points in a bounding square and determining the ratio of points that fall inside the circle.\n\n``` {python}\n#| code-fold: true\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to check if a point is inside the shape (e.g., a circle)\ndef is_inside_circle(x, y):\n    return x**2 + y**2 <= 1\n\n# Function to run Monte Carlo simulations and visualize the points\ndef visualize_monte_carlo(num_points):\n    inside_points_x = []\n    inside_points_y = []\n    outside_points_x = []\n    outside_points_y = []\n\n    for _ in range(num_points):\n        x = np.random.uniform(-1, 1)\n        y = np.random.uniform(-1, 1)\n\n        if is_inside_circle(x, y):\n            inside_points_x.append(x)\n            inside_points_y.append(y)\n        else:\n            outside_points_x.append(x)\n            outside_points_y.append(y)\n\n    # Plotting the results\n    plt.figure(figsize=(8, 8))\n    plt.scatter(outside_points_x, outside_points_y, color='blue', label='Outside Shape')\n    plt.scatter(inside_points_x, inside_points_y, color='red', label='Inside Shape')\n    plt.title(f'Monte Carlo Simulation: Points Inside and Outside a Circle')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.legend()\n    plt.axis('equal')\n    plt.grid(True)\n    plt.show()\n\n    return inside_points_x\n\n# Visualize the Monte Carlo simulation with 1000 points\nnum_points = 1000\ninside_points_x = visualize_monte_carlo(num_points)\n\n```\n<p style='font-size:23px'>\n\n## Results\n\nAfter running the Monte Carlo simulation with 1000 points, we can calculate the estimated area of the circle with.. $$ Estimated Area = ( Points Inside Circle / Total Points ) * Area of Square $$\nThis estimate should converge on the value of pi, becoming more accurate as you add more points to the simulation!\n\n``` {python}\n#| code-fold: true\n\n# Calculate the estimated area using the ratio of points inside the circle\nestimated_area = (len(inside_points_x) / num_points) * 4  # Area of the bounding square is 4\n\n# Print the estimated area\nprint(f\"Estimated Area of the Circle: {estimated_area:.4f}\")\n\n```\n<p style='font-size:23px'>\n\n\n\nImagine scenarios where relationships between spaces and subspaces defy direct mathematical representation—here, Monte Carlo simulations shine. By generating random points and gauging their distribution, we can estimate dimensions, areas, and probabilities with remarkable accuracy.\n\nThe applications are vast and varied. In finance, Monte Carlo simulations aid risk assessment and portfolio optimization. In physics, they model particle interactions. For logistics, they optimize supply chain decisions. In machine learning, their adaptability shines—training data augmentation, uncertainty estimation, and reinforcement learning all benefit from the Monte Carlo approach.\n\nThank you!","srcMarkdownNoYaml":"\n\n## Introduction\n\nIn the context of probability theory, random variables, and engineering in general, Monte Carlo simulations stand as powerful navigational tools, offering a unique lens through which we can explore and understand the uncertainties inherent in various phenomena. Named after the illustrious Monte Carlo Casino in Monaco, where chance plays a central role, these simulations harness the power of randomness to estimate probabilities and model complex systems. Through the art of random sampling, we'll unravel the mysteries of chance, casting a digital die to uncover patterns, distributions, and showcase how this technique can be used to approximate values that are otherwise vary difficult to ascertain.\n\n## Monte Carlo Simulation Simple Demonstration\n\nNow, let's embark on a hands-on journey to demystify the magic of probability theory and random variables using a simple yet powerful example: estimating the probability of getting heads in a fair coin toss. \n\nPicture yourself holding a fair coin. When you flip it, the outcome is uncertain – it could be heads or tails, a classic example of a random variable. The intrigue lies in understanding the behavior of this coin, and that's where probability theory steps in.\n\nIn the world of probability theory, we're curious about the likelihood of specific outcomes. Here, our burning question is: What's the probability of landing heads in a single toss? This is where Monte Carlo simulations come into play – a dynamic tool that transforms uncertainty into insight.\n\nImagine taking that fair coin and flipping it not just once, but thousands of times in a virtual environment. Each flip is like a mini-experiment, and the more experiments we conduct, the better we can grasp the elusive nature of our random variable – the coin toss. Monte Carlo simulations let us explore this randomness by simulating scenarios, recording outcomes, and gradually revealing the underlying probabilities.\n\nHere we run the simulation of the coin toss 10,000 times resulting in a relatively accurate estimation of what the probability should theoretically be.\n\n\n<p style='font-size:23px'>\n&emsp;&emsp; \n</p>\n``` {python}\n#| code-fold: true\nimport numpy as np\n\n# Function to simulate a coin toss\ndef simulate_coin_toss(num_simulations):\n    outcomes = np.random.choice(['Heads', 'Tails'], size=num_simulations)\n    heads_count = np.sum(outcomes == 'Heads')\n    probability_heads = heads_count / num_simulations\n    return probability_heads\n\n# Number of simulations\nnum_simulations = 10000\n\n# Run the Monte Carlo simulation\nestimated_probability = simulate_coin_toss(num_simulations)\n\nprint(f\"Estimated Probability of Heads: {estimated_probability}\")\n```\n<p style='font-size:23px'>\n\nNow, let's visualize how our estimated probability converges to the true theoretical probability as we increase the number of simulations. In this demonstration, the number of simulations increases by an order of magnitude each iteration which can be more approachably shown by graphing the x-axis on a logarithmic scale. \n\n``` {python}\n#| code-fold: true\nimport matplotlib.pyplot as plt\n\n# Function to run Monte Carlo simulations with varying sample sizes\ndef run_simulations():\n    num_simulations_list = [10, 100, 1000, 10000, 100000, 1000000]\n    probabilities = []\n\n    for num_simulations in num_simulations_list:\n        estimated_probability = simulate_coin_toss(num_simulations)\n        probabilities.append(estimated_probability)\n\n    return num_simulations_list, probabilities\n\n# Run simulations\nnum_simulations_list, probabilities = run_simulations()\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.xscale('log')\nplt.plot(num_simulations_list, probabilities, marker='o', linestyle='-', color='b')\nplt.axhline(y=0.5, color='r', linestyle='--', label='Theoretical Probability (0.5)')\nplt.title('Convergence of Monte Carlo Simulation to Theoretical Probability')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Estimated Probability of Heads')\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n<p style='font-size:23px'>\n\nIt is important to note that everytime you run a monte carlo simulation, the results will vary due to the stochastic nature of the process. Above you should see that, generally, as the number of simulations increases so does the accuracy of the estimation. However, other factors may affect the convergence of the estimate such as initial biases, statistical variability, and the nature of the specific problem you are applying the algorithm to which may skew the reliability of the results. \n\n## Monte Carlo Simulation: Estimating Circle Area\n\nNext, we can apply a monte carlo simulation technique to a more interesting problem: estimating the area of a shape. This could be useful in situations where you have a space and a subspace and it is difficult or unideal to model their relationship mathematically. In general, you can generate random points within the space and use the ratio of points that exist in the subspace to the points that exist in the entire space in order to estimate the dimensions of the subspace. For sake of simpilicity, here we can demonstrate this for a circle.\n\nConsider a circle with radius.. $$ r = 1 $$ ..centered at the origin. The formula for the area of a circle is given by..  $$ A = pi*r^2 $$ We can estimate this area by generating random points in a bounding square and determining the ratio of points that fall inside the circle.\n\n``` {python}\n#| code-fold: true\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to check if a point is inside the shape (e.g., a circle)\ndef is_inside_circle(x, y):\n    return x**2 + y**2 <= 1\n\n# Function to run Monte Carlo simulations and visualize the points\ndef visualize_monte_carlo(num_points):\n    inside_points_x = []\n    inside_points_y = []\n    outside_points_x = []\n    outside_points_y = []\n\n    for _ in range(num_points):\n        x = np.random.uniform(-1, 1)\n        y = np.random.uniform(-1, 1)\n\n        if is_inside_circle(x, y):\n            inside_points_x.append(x)\n            inside_points_y.append(y)\n        else:\n            outside_points_x.append(x)\n            outside_points_y.append(y)\n\n    # Plotting the results\n    plt.figure(figsize=(8, 8))\n    plt.scatter(outside_points_x, outside_points_y, color='blue', label='Outside Shape')\n    plt.scatter(inside_points_x, inside_points_y, color='red', label='Inside Shape')\n    plt.title(f'Monte Carlo Simulation: Points Inside and Outside a Circle')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.legend()\n    plt.axis('equal')\n    plt.grid(True)\n    plt.show()\n\n    return inside_points_x\n\n# Visualize the Monte Carlo simulation with 1000 points\nnum_points = 1000\ninside_points_x = visualize_monte_carlo(num_points)\n\n```\n<p style='font-size:23px'>\n\n## Results\n\nAfter running the Monte Carlo simulation with 1000 points, we can calculate the estimated area of the circle with.. $$ Estimated Area = ( Points Inside Circle / Total Points ) * Area of Square $$\nThis estimate should converge on the value of pi, becoming more accurate as you add more points to the simulation!\n\n``` {python}\n#| code-fold: true\n\n# Calculate the estimated area using the ratio of points inside the circle\nestimated_area = (len(inside_points_x) / num_points) * 4  # Area of the bounding square is 4\n\n# Print the estimated area\nprint(f\"Estimated Area of the Circle: {estimated_area:.4f}\")\n\n```\n<p style='font-size:23px'>\n\n\n\nImagine scenarios where relationships between spaces and subspaces defy direct mathematical representation—here, Monte Carlo simulations shine. By generating random points and gauging their distribution, we can estimate dimensions, areas, and probabilities with remarkable accuracy.\n\nThe applications are vast and varied. In finance, Monte Carlo simulations aid risk assessment and portfolio optimization. In physics, they model particle interactions. For logistics, they optimize supply chain decisions. In machine learning, their adaptability shines—training data augmentation, uncertainty estimation, and reinforcement learning all benefit from the Monte Carlo approach.\n\nThank you!"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title":"Exploring Probabilities with Monte Carlo Simulations","author":"Stephen Owesney","date":"2023-12-13","categories":["code","ML","probability","monte carlo"],"image":"data.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}