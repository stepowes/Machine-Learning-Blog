{
  "hash": "e9125a7a3f130a4a2cb9088890ea8df9",
  "result": {
    "markdown": "---\ntitle: \"Classification with KNNs\"\nauthor: \"Stephen Owesney\"\ndate: \"2023-12-13\"\ncategories: [code, ML, classification, KNN, Confusion Matrix]\nimage: \"classification.png\"\n---\n\n## Introduction\n\nClassification models play a pivotal role in assigning labels to data points, facilitating decision-making in various domains. As we delve into the nuances of classification, this blog post will explain essential metrics—Receiver Operating Characteristic (ROC) curves, Precision-Recall (PR) curves, and Confusion Matrix. Together, they offer profound insights into the performance of classification models, guiding us through the complex terrain of true positives, false positives, precision, recall, and the elusive trade-offs between them.\n\n## Understanding Relevant Classification Metrics\n\n### Receiver Operating Characteristic (ROC) Curve\n\nThe Receiver Operating Characteristic (ROC) curve is a fundamental tool in the evaluation of classification models. It illustrates the performance of a binary classifier by plotting the true positive rate against the false positive rate across various threshold settings. The curve provides valuable insights into the model's ability to distinguish between classes, specifically showcasing the trade-off between sensitivity and specificity. Understanding the ROC curve involves interpreting the area under the curve (AUC), where a higher AUC signifies superior model performance. \n\n### Precision-Recall (PR) Curve\n\nIn the landscape of classification evaluation, the Precision-Recall (PR) curve complements the insights gained from the ROC curve. Unlike the ROC curve that focuses on true positive rates and false positive rates, the PR curve emphasizes the trade-off between precision and recall. Precision, a measure of the accuracy of positive predictions, and recall, a measure of the model's ability to capture all positive instances, take center stage in the PR curve analysis. The PR curve serves as a valuable tool, especially in scenarios where imbalanced class distribution demands a closer examination of positive class prediction accuracy.\n\n### Confusion Matrix\n\nThe Confusion Matrix stands as a cornerstone in the assessment of classification model performance. Comprising four key metrics—true positives, true negatives, false positives, and false negatives—the matrix provides a comprehensive view of the model's predictive capabilities. Each cell in the matrix represents a different aspect of the model's predictions, allowing us to quantify accuracy, precision, recall, and the F1 score.\n\nAs we delve into the Confusion Matrix section, we will explore how to interpret these metrics and understand the implications for decision-making. The Confusion Matrix is an invaluable tool for gaining a nuanced understanding of a model's strengths and weaknesses, guiding us in refining our approach to classification challenges.\n\n## Applying Classification: K Nearest Neighbors (KNN)\n\nTo put these classification metrics into action, let's explore the application of the K Nearest Neighbors (KNN) algorithm—a versatile and intuitive classifier. KNN is a type of instance-based learning where a data point is classified by the majority class of its k nearest neighbors.\n\n### Dataset: Iris Flowers\n\nWe'll employ the classic Iris dataset, a well-known benchmark for classification tasks. This dataset comprises measurements of sepal length, sepal width, petal length, and petal width for three species of iris flowers—setosa, versicolor, and virginica.\n\n### KNN Implementation\n\nLet's implement the KNN classifier using the scikit-learn library and showcase the classification results with a visual representation.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load the Iris dataset (replace this with your own data loading)\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the KNN classifier\nknn_classifier = KNeighborsClassifier(n_neighbors=3)  # You can adjust the number of neighbors as needed\nknn_classifier.fit(X_train, y_train)\n\n# Get predictions\ny_pred = knn_classifier.predict(X_test)\n\n# Calculate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix using seaborn\nplt.figure(figsize=(8, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=iris.target_names, yticklabels=iris.target_names)\nplt.title('Confusion Matrix: K Nearest Neighbors (KNN)')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=655 height=671}\n:::\n:::\n\n\n<p style='font-size:23px'>\n\nThe confusion matrix serves as a visual representation of the performance of our K Nearest Neighbors (KNN) classifier on the Iris dataset. Each row of the matrix corresponds to the actual class, while each column represents the predicted class. The diagonal elements of the matrix indicate the number of correctly classified instances for each class. This insightful tool allows us to analyze where our classifier excels and where it may encounter challenges. By breaking down predictions into true positives, true negatives, false positives, and false negatives, we gain a nuanced understanding of the classifier's strengths and areas for improvement. This analysis forms the foundation for interpreting more advanced metrics like precision, recall, ROC curves, and PR curves, providing a comprehensive evaluation of our classification model.\n\n\nAs demonstrated through our exploration of the KNN algorithm on the Iris dataset, classification techniques provide a robust framework for decision-making in diverse domains. Whether discerning between species of flowers or making critical decisions in complex scenarios, the power of classification lies in its ability to distill patterns from data, guiding us toward informed choices. KNN is a very basic and introductory technique in classification. As we continue to refine and advance classification techniques, the role they play remain at the foundation of classification.\n\nThank you!\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}